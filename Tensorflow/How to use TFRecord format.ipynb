{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use TFRecord format\n",
    "Tensorflow에서 지원하는 data format인 `TFRecord` format으로 data를 converting하고 아룰 `tf.data` 를 사용하여 load하는 방법에 대해서 정리, `TFRecord` format의 특징은 아래와 같음. mnist dataset을 `TFRecord` format으로 converting하고, 이를 `tf.data` 로 읽어들이는 예제\n",
    "\n",
    "* Property\n",
    "    + `TFRecord` format의 dataset은 하나의 memory block에 저장되므로, file이 개별로 저장되었을 경우에 비해 io가 빠르다.\n",
    "    + multi-thread로 활용하기에 더 적합\n",
    "* Reference\n",
    "    + <https://www.tensorflow.org/api_docs/python/tf/python_io>\n",
    "    + <https://www.tensorflow.org/api_guides/python/python_io>\n",
    "    + <http://bcho.tistory.com/1190> \n",
    "    + <https://medium.com/trackin-datalabs/convert-to-tfrecords-dataset-2087b0ffa4f5>\n",
    "    + <http://excelsior-cjh.tistory.com/161>\n",
    "\n",
    "### Converting dataset to TFRecord format\n",
    "dataset을 `TFRecord` format으로 바꾸는 과정에서 필요한 api는 아래와 같다.\n",
    "\n",
    "* `tf.python_io.TFRecordWriter` class의 instance를 생성 \n",
    "    + argument\n",
    "        - path : `TFRecord` format으로 쓸 경로, 확장자는 `.tfrecords`\n",
    "        - options : `tf.python_io.TFRecordOptions` class의 instance를 전달, 예제 코드에서는\n",
    "\n",
    "    ```python\n",
    "    options = tf.python_io.TFRecordOptions(compression_type = tf.python_io.TFRecordCompressionType.GZIP)\n",
    "    ```\n",
    "\n",
    "    + usage\n",
    "        -  생성된 instance에서 `write` method를 이용하여, `tf.train.Example` 로 생성된 인스턴스를 path에 전달한 경로 parameter에 write한다.\n",
    "\n",
    "\n",
    "* `tf.train.Example` class의 instance를 생성\n",
    "    + argument\n",
    "        - Features : `tf.train.Features` class의 instance를 전달\n",
    "            - `tf.train.Features` 의 argument는 feature로 feature에 `tf.train.Feature` class의 instance가 `dictionary` type으로 모여진 `dictionary` 를 전달하여 instance를 생성한다. 예제 코드에서는 \n",
    "\n",
    "    ```python\n",
    "    example = tf.train.Example(features = tf.train.Features(feature = {\n",
    "                'label' : tf.train.Feature(int64_list = tf.train.Int64List(value = [label])),\n",
    "                'image' : tf.train.Feature(bytes_list = tf.train.BytesList(value = [image]))}))\n",
    "    ```\n",
    "\n",
    "    + usage\n",
    "        - 생성된 instance에서 `SerializeToString` method를 이용하여 직렬화하고, 이를 `tf.python_io.TFRecordWriter` class에서 생성된 instance의 `write` method로 write한다.\n",
    "        \n",
    "### Loading dataset converted TFRecord with `tf.data`\n",
    "`TFRecord` format의 dataset을 `tf.data` 를 이용하여 불러오는 과정에서 필요한 api는 아래와 같다.\n",
    "\n",
    "* `tf.data.TFRecordDataset` class의 instance를 생성\n",
    "    + argument\n",
    "        - filenames : `TFRecord` format의 dataset의 경로\n",
    "        - compression_type : 불러오는 `TFRecord` format dataset의 compression type을 전달\n",
    "    + usage\n",
    "        - 생성된 instance에서 `map` method를 활용, 이 때 `TFRecord` format을 parsing하는 함수를 작성하고 `lambda` 로 선언한 anonymous function을 이용한다. 예제 코드에서는\n",
    "\n",
    "    ```python\n",
    "    def parse_single_example(record):\n",
    "        features = {'label' : tf.FixedLenFeature((), tf.int64, 0),\n",
    "                    'image' : tf.FixedLenFeature((), tf.string, '')}\n",
    "        parsed_features = tf.parse_single_example(serialized = record, features = features)\n",
    "        image = tf.decode_raw(parsed_features.get('image'), out_type = tf.float32)\n",
    "        image = tf.reshape(tensor = image, shape = [28,28,1])\n",
    "        label = tf.cast(parsed_features.get('label'), dtype = tf.int32)\n",
    "        return image, label\n",
    "    ```\n",
    "\n",
    "    ```python\n",
    "    val = tf.data.TFRecordDataset(filenames = './mnist_validation.tfrecords', compression_type = 'GZIP')\n",
    "    val = val.map(lambda record : parse_single_example(record))\n",
    "    ```\n",
    "    \n",
    "    + 이후 과정은 `tf.data` 를 일반적으로 사용하는 방식과 같다. 아래의 링크를 참고\n",
    "        - [How to simply use tf.data](https://github.com/aisolab/TIL/blob/master/Tensorflow/How%20to%20simply%20use%20tf.data.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Split\n",
    "`TFRecord` format으로 변환할 dataset인 mnist dataset을 load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28, 1) (10000,) (50000, 28, 28, 1) (10000,)\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(dtype = np.float32).reshape(-1,28,28,1)\n",
    "y_train = y_train.astype(dtype = np.int32)\n",
    "X_test = X_test.astype(dtype = np.float32).reshape(-1,28,28,1)\n",
    "y_test = y_test.astype(dtype = np.int32)\n",
    "\n",
    "# training data에서 10000개의 데이터를 뽑음\n",
    "val_indices = np.random.choice(range(X_train.shape[0]), size = 10000, replace = False)\n",
    "X_val = X_train[val_indices] \n",
    "y_val = y_train[val_indices]\n",
    "X_train = np.delete(arr = X_train, obj = val_indices, axis = 0)\n",
    "y_train = np.delete(arr = y_train, obj = val_indices, axis = 0)\n",
    "print(X_val.shape, y_val.shape, X_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting dataset to TFRecord format\n",
    "mnist example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = zip(X_train, y_train)\n",
    "validation = zip(X_val, y_val)\n",
    "test = zip(X_test, y_test)\n",
    "\n",
    "split = dict(train = train, validation = validation, test = test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train was converted to tfrecords\n",
      "validation was converted to tfrecords\n",
      "test was converted to tfrecords\n"
     ]
    }
   ],
   "source": [
    "options = tf.python_io.TFRecordOptions(compression_type = tf.python_io.TFRecordCompressionType.GZIP)\n",
    "\n",
    "for key in split.keys():\n",
    "    dataset = split.get(key)\n",
    "    writer = tf.python_io.TFRecordWriter(path = './mnist_{}.tfrecords'.format(key), options = options)\n",
    "    \n",
    "    for data, label in dataset:\n",
    "        image = data.tostring()\n",
    "        example = tf.train.Example(features = tf.train.Features(feature = {\n",
    "            'label' : tf.train.Feature(int64_list = tf.train.Int64List(value = [label])),\n",
    "            'image' : tf.train.Feature(bytes_list = tf.train.BytesList(value = [image]))\n",
    "        }))\n",
    "        writer.write(example.SerializeToString())\n",
    "    else:\n",
    "        writer.close()\n",
    "        print('{} was converted to tfrecords'.format(key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset converted TFRecord with `tf.data`\n",
    "mnist example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_single_example(record):\n",
    "    features = {'label' : tf.FixedLenFeature((), tf.int64, 0),\n",
    "                'image' : tf.FixedLenFeature((), tf.string, '')}\n",
    "    parsed_features = tf.parse_single_example(serialized = record, features = features)\n",
    "    image = tf.decode_raw(parsed_features.get('image'), out_type = tf.float32)\n",
    "    image = tf.reshape(tensor = image, shape = [28,28,1])\n",
    "    label = tf.cast(parsed_features.get('label'), dtype = tf.int32)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = tf.data.TFRecordDataset(filenames = './mnist_validation.tfrecords', compression_type = 'GZIP')\n",
    "val = val.map(lambda record : parse_single_example(record))\n",
    "val = val.batch(batch_size = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_iterator = val.make_initializable_iterator()\n",
    "X, y = val_iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(val_iterator.initializer)\n",
    "    X_data = sess.run(X)\n",
    "    y_data = sess.run(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 28, 28, 1) float32\n",
      "(2,) int32\n"
     ]
    }
   ],
   "source": [
    "print(X_data.shape, X_data.dtype)\n",
    "print(y_data.shape, y_data.dtype)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
